# Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation

<p align="center">
  <img src="https://github.com/user-attachments/assets/12d44362-8b7f-4952-9488-0e45cf759b57" alt="teaser"/>
</p>

**TL;DR: Feed-forward 3D and 4D scene generation from a single image/video trained with synthetic data generated by a camera-controlled video diffusion model.**

**Full Abstract**:
The ability to generate virtual environments is crucial for applications ranging from gaming to physical AI domains such as robotics, autonomous driving, and industrial AI. Current learning-based 3D reconstruction methods rely on the availability of captured real-world multi-view data, which is not always readily available. Recent advancements in video diffusion models have shown remarkable imagination capabilities, yet their 2D nature limits the applications to simulation where a robot needs to navigate and interact with the environment. In this paper, we propose a self-distillation framework that aims to distill the implicit 3D knowledge in the video diffusion models into an explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for multi-view training data. Specifically, we augment the typical RGB decoder with a 3DGS decoder, which is supervised by the output of the RGB decoder. In this approach, the 3DGS decoder can be purely trained with synthetic data generated by video diffusion models. At inference time, our model can synthesize 3D scenes from either a text prompt or a single image for real-time rendering. Our framework further extends to dynamic 3D scene generation from a monocular input video. Experimental results show that our framework achieves state-of-the-art performance in static and dynamic 3D scene generation.

**[Paper](https://arxiv.org/abs/2509.19296), [Project Page](https://research.nvidia.com/labs/toronto-ai/lyra/), [Dataset](https://huggingface.co/datasets/nvidia/PhysicalAI-SpatialIntelligence-Lyra-SDG), [Model Weights](https://huggingface.co/nvidia/Lyra)**

[Sherwin Bahmani](https://sherwinbahmani.github.io/),
[Tianchang Shen](https://www.cs.toronto.edu/~shenti11/),
[Jiawei Ren](https://jiawei-ren.github.io/),
[Jiahui Huang](https://huangjh-pub.github.io/),
[Yifeng Jiang](https://cs.stanford.edu/~yifengj/),
[Haithem Turki](https://haithemturki.com/),
[Andrea Tagliasacchi](https://theialab.ca/),
[David B. Lindell](https://davidlindell.com/),
[Zan Gojcic](https://zgojcic.github.io/),
[Sanja Fidler](https://www.cs.utoronto.ca/~fidler/),
[Huan Ling](https://www.cs.toronto.edu/~linghuan/),
[Jun Gao](https://www.cs.toronto.edu/~jungao/),
[Xuanchi Ren](https://xuanchiren.com/) <br>

## Installation

Please follow the [INSTALL.md](INSTALL.md) to set up your conda environment and download pre-trained weights.

## Demo
Lyra supports both images and videos as input. Below are examples of running Lyra on single images and videos.

First, you need to download the demo samples:

```bash
# Download test samples from Hugging Face
huggingface-cli download nvidia/Lyra-Testing-Example --repo-type dataset --local-dir assets/demo
```

### Example 1: Single Image to 3D Gaussians Generation

1) Generate multi-view video latents from the input image using scripts/bash/static_sdg.sh. 

```bash
CUDA_HOME=$CONDA_PREFIX PYTHONPATH=$(pwd) torchrun --nproc_per_node=1 cosmos_predict1/diffusion/inference/gen3c_single_image_sdg.py \
    --checkpoint_dir checkpoints \
    --num_gpus 1 \
    --input_image_path assets/demo/static/diffusion_input/images/00172.png \
    --video_save_folder assets/demo/static/diffusion_output_generated \
    --foreground_masking \
    --multi_trajectory \
    --total_movement_distance_factor 1.0
```

Increase total_movement_distance_factor to 2.0 for more camera motion, though it can create more artifacts in object-centric scenes. If you want to skip the diffusion part, we have pre-generated the latents in assets/demo/static/diffusion_output. By default we use pre-generated latents, change dataset_name in configs/demo/lyra_static.yaml from lyra_static_demo to lyra_static_demo_generated to use your own generated latents.

2) Reconstruct multi-view video latents with the 3DGS decoder (change dataset_name in the .yaml to generated path if 1. was done)

```bash
accelerate launch sample.py --config configs/demo/lyra_static.yaml
```

### Example 2: Single Video to Dynamic 3D Gaussians Generation

1) Generate multi-view video latents from the input video and ViPE estimated depth using scripts/bash/dynamic_sdg.sh.

```bash
CUDA_HOME=$CONDA_PREFIX PYTHONPATH=$(pwd) torchrun --nproc_per_node=1 cosmos_predict1/diffusion/inference/gen3c_dynamic_sdg.py \
    --checkpoint_dir checkpoints \
    --vipe_path assets/demo/dynamic/diffusion_input/rgb/6a71ee0422ff4222884f1b2a3cba6820.mp4 \
    --video_save_folder assets/demo/dynamic/diffusion_output_generated \
    --disable_prompt_upsampler \
    --num_gpus 1 \
    --foreground_masking \
    --multi_trajectory
```

If you want to skip the diffusion part, we have pre-generated the latents in assets/demo/dynamic/diffusion_output. By default we use pre-generated latents, change dataset_name in configs/demo/lyra_dynamic.yaml from lyra_dynamic_demo to lyra_dynamic_demo_generated to use your own generated latents.
Add --flip_supervision if you want to also generate the motion reversed training data (not needed for inference).

2) Reconstruct multi-view video latents with the 3DGS decoder (change dataset_name in the .yaml to generated path if 1. was done)

```bash
accelerate launch sample.py --config configs/demo/lyra_dynamic.yaml
```

#### Testing on your own videos using ViPE
Follow the installation instructions for [ViPE](https://github.com/nv-tlabs/vipe). Note: ViPE's environment is not compatible with Lyra. We recommend installing ViPE in a separate conda environment. The ViPE results are required for dynamic scene generation. Moreover, we use the depth from ViPE for depth supervision during 3DGS decoder training.

1) Run ViPE to extract depth, intrinsics, and camera poses (make sure to use the --lyra flag to use the same depth estimator as us):
```bash
vipe infer YOUR_VIDEO.mp4 -p lyra --output <vipe_results_dir>
```

2) Define the new data path in src/models/data/registry.py as dataset following the structure of our provided datasets

### GPU Memory Requirements

We have tested Lyra only on H100 and A100 GPUs. For GPUs with limited memory, you can fully offload all models by appending the following flags to your SDG command:

```bash
--offload_diffusion_transformer \
--offload_tokenizer \
--offload_text_encoder_model \
--offload_prompt_upsampler \
--offload_guardrail_models \
--disable_guardrail \
--disable_prompt_encoder
```
Maximum observed memory during inference with full offloading: ~43GB. Note: Memory usage may vary depending on system specifications and is provided for reference only.

## Training

We provide training scripts to train from scratch or fine-tune our models. First, you need to download our [training data](https://huggingface.co/datasets/nvidia/PhysicalAI-SpatialIntelligence-Lyra-SDG):

```bash
# Download our training datasets from Hugging Face and untar them into a static/dynamic folder
huggingface-cli download nvidia/PhysicalAI-SpatialIntelligence-Lyra-SDG --repo-type dataset --local-dir lyra_dataset/tar
```

Alternatively, use the demo script to generate training data. Here, the diffusion part is sufficient without running the 3DGS decoder, since we want to train that. Make sure to update the paths in src/models/data/registry.py for lyra_static / lyra_dynamic to wherever your data is stored. We provide our progressive training script:

```bash
bash train.sh
```

We provide visualization scripts during training to export renderings and 3D Gaussians for each stage:

```bash
bash inference.sh
```

### Colab Demo
Try Lyra directly in your browser with this [Community Colab notebook](https://colab.research.google.com/drive/153rmtsQskjsxIdi5xx8m4SwniKeeWBbx?usp=sharing).
- Loads official demo data.
- Supports custom single-image upload.
- Generates 3D Gaussian Splats end-to-end.

## Acknowledgement
Our model is based on [NVIDIA Cosmos](https://github.com/NVIDIA/Cosmos) and [GEN3C](https://github.com/nv-tlabs/GEN3C). We use input images generated by [FLUX](https://github.com/black-forest-labs/flux).

We are also grateful to several other open-source repositories that we drew inspiration from or built upon during the development of our pipeline:
- [MoGe](https://github.com/microsoft/MoGe)
- [TrajectoryCrafter](https://github.com/TrajectoryCrafter/TrajectoryCrafter)
- [DimensionX](https://github.com/wenqsun/DimensionX)
- [Depth Anything V2](https://github.com/DepthAnything/Depth-Anything-V2)
- [Video Depth Anything](https://github.com/DepthAnything/Video-Depth-Anything)
- [Long-LRM](https://github.com/arthurhero/Long-LRM)

## Citation
```
@inproceedings{bahmani2025lyra,
   title={Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation},
   author={Bahmani, Sherwin and Shen, Tianchang and Ren, Jiawei and Huang, Jiahui and Jiang, Yifeng and 
            Turki, Haithem and Tagliasacchi, Andrea and Lindell, David B. and Gojcic, Zan and Fidler, Sanja and 
            Ling, Huan and Gao, Jun and Ren, Xuanchi},
   booktitle={arXiv preprint arXiv:2509.19296},
   year={2025}
}
```

## License and Contact

This project will download and install additional third-party open source software projects. Review the license terms of these open source projects before use.

Lyra source code is released under the [Apache 2 License](https://www.apache.org/licenses/LICENSE-2.0).

Lyra models are released under the [NVIDIA Open Model License](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license). For a custom license, please visit our website and submit the form: [NVIDIA Research Licensing](https://www.nvidia.com/en-us/research/inquiries/).
